
@article{koning_identification_2015,
	title = {Identification of patients at risk for colorectal cancer in primary care: an explorative study with routine healthcare data},
	volume = {27},
	issn = {0954-691X},
	doi = {10.1097/MEG.0000000000000472},
	abstract = {Background Early diagnosis of colorectal cancer (CRC) is likely to reduce burden of disease and improve treatment success. Estimation of the individual patient risk for CRC diagnostic determinants in a primary care setting has not been very successful as yet. The aim of our study is to improve prediction of CRC in patients selected for colonoscopy in the primary healthcare setting using readily available routine healthcare data. Patients and methods A cross-sectional study was carried out in the Julius General Practitioners' Network database. Patients referred for colonoscopy by their general practitioner (GP) between 2007 and 2012 were selected. We evaluated the association between long-term registered patient characteristics, symptoms and conditions, and colonoscopy test results with multivariable logistic regression. Results Two per cent (2787/140 000) of the patients between 30 and 85 years were found to be newly referred for colonoscopy by their GP, of whom 57 (2\%) were diagnosed with CRC. Age 50 years or over, hypertension and the absence of preceding consultations for abdominal pain were independent predictors for CRC and/or high-risk adenomas, with an area under the curve of 0.65. Conclusion Three factors in routine care data combined might prove valuable in future strategies to improve the prediction of CRC risk in primary care. Improvement in quality and availability of routine care data for research and risk stratification is needed to optimize its usability for prediction purposes in daily practice. Impact Only referring patients at the highest risk for colonoscopy by the GP could decrease superfluous colonoscopies.},
	number = {12},
	journal = {European Journal of Gastroenterology and Hepatology},
	author = {Koning, N. R. and Moons, L. N. G. and Büchner, F. L. and Helsper, C. W. and ten Teije, A.C.M. and Numans, M. E.},
	year = {2015},
	note = {Publisher: Lippincott Williams and Wilkins},
	pages = {1443--1448},
}

@incollection{hoekstra_hubble_2015,
	series = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	title = {Hubble: {Linked} data {Hub} for clinical decision support},
	volume = {7540},
	isbn = {978-3-662-46640-7},
	abstract = {The AERS datasets is one of the few remaining, large publicly available medical data sets that until now have not been published as Linked Data. It is uniquely positioned amidst othermedical datasets. This paper describes the Hubble prototype system for clinical decision support that demonstrates the speed, ease and flexibility of producing and using a Linked Data version of the AERS dataset for clinical practice and research.},
	booktitle = {The {Semantic} {Web}: {ESWC} 2012 {Satellite} {Events} - {Revised} {Selected} {Papers}},
	publisher = {Springer/Verlag},
	author = {Hoekstra, Rinke and Magliacane, Sara and Rietveld, Laurens and De Vries, Gerben and Wibisono, Adianto and Schlobach, Stefan},
	year = {2015},
	doi = {10.1007/978-3-662-46641-4_45},
	keywords = {Adverse event, Clinical decision support, Health care, Linked data},
	pages = {458--462},
}

@misc{beek_frank_2015,
	title = {Frank: {The} {LOD} cloud at your fingertips?},
	abstract = {Large-scale, algorithmic access to LOD Cloud data has been hampered by the absence of queryable endpoints for many datasets, a plethora of serialization formats, and an abundance of idiosyncrasies such as syntax errors. As of late, very large-scale — hundreds of thousands of document, tens of billions of triples — access to RDF data has become possible thanks to the LOD Laundromat Web Service. In this paper we showcase Frank, a command-line interface to a very large collection of standards-compliant, real-world RDF data that can be used to run Semantic Web experiments and stress-test Linked Data applications.},
	author = {Beek, Wouter and Rietveld, Laurens},
	year = {2015},
}

@article{zamborlini_inferring_2015,
	title = {Inferring {Recommendation} {Interactions} in {Clinical} {Guidelines}: {Case}-studies on {Multimorbidity}},
	issn = {1570-0844},
	journal = {Semantic Web},
	author = {Zamborlini, V. and Hoekstra, Rinke and da Silveira, M. and Pruski, C. and ten Teije, A.C.M.},
	year = {2015},
	note = {Publisher: IOS Press},
}

@incollection{huang_identifying_2015,
	title = {Identifying {Evidence} {Quality} for {Updating} {Evidence}-based {Medical} {Guidelines}},
	isbn = {978-3-319-26584-1},
	booktitle = {Knowledge {Representation} for {Health} {Care}, {AIME} 2015 {International} {Joint} {Workshop}, {KR4HC}/{ProHealth} 2015},
	publisher = {Springer},
	author = {Huang, Z. and Hu, Q. and ten Teije, A.C.M. and van Harmelen, F.A.H.},
	editor = {Riano, D. and Lenz, R. and Miksch, S. and Peleg, M. and Reichert, M. and ten Teije, A.C.M.},
	year = {2015},
	pages = {51--64},
}

@incollection{huang_identifying_2015-1,
	title = {Identifying {Evidence} {Quality} for {Updating} {Evidence}-based {Medical} {Guideline}},
	booktitle = {Proceedings of {International} {Joint} {WorkshopKR4HC} 2015 - {ProHealth} 2015},
	author = {Huang, Z. and Hu, Q. and ten Teije, A. and van Harmelen, F.},
	year = {2015},
}

@incollection{beek_frank_2015-1,
	title = {Frank: {Algorithmic} {Access} to the {LOD} {Cloud}},
	booktitle = {Proceedings of the {Developer}'s {Workshop} {ESWC} 2015},
	author = {Beek, W. G. J. and Rietveld, L. J.},
	year = {2015},
	pages = {41--46},
}

@incollection{magliacane_foxpsl_2015,
	title = {{FoxPSL}: {An} {Extended} and {Scalable} {PSL} {Implementation}},
	booktitle = {{AAAI} {Spring} {Symposium} 2015 on {Knowledge} {Representation} and {Reasoning}},
	author = {Magliacane, S. and Stutz, P. and Groth, P. and Bernstein, A.},
	year = {2015},
}

@incollection{reinders_finding_2015,
	title = {Finding {Evidence} for {Updates} in {Medical} {Guidelines}},
	abstract = {Medical guidelines are documents that describe optimal treatment for patients by medical practitioners based on current medical research (evidence), in the form of step-by-step recommendations. Because the field of medical research is very large and always evolving, keeping these guidelines up-to-date with the current state of the art is a difficult task. In this paper, we propose a method for finding relevant evidence for supporting the medical guideline updating process. Our method that takes from the evidence-based medical guideline the recommendations and their corresponding evidence as its input, and that queries PubMed, the world's largest search engine for medical citations, for potential new or improved evidence. We built a prototype and performed a feasibility study on a set of old recommendations, and compared the output to evidence for the newer version. The system succeeded in finding goal articles for 11 out of 16 recommendations, but in total, only 20 out of 71 articles were retrieved. Our ranking method for most relevant articles worked well for small result sets, but for large result sets it failed to rank the goal articles in the top 25 results.},
	booktitle = {{HEALTHINF} 2015 - 8th {International} {Conference} on {Health} {Informatics}, {Proceedings}; {Part} of 8th {International} {Joint} {Conference} on {Biomedical} {Engineering} {Systems} and {Technologies}, {BIOSTEC} 2015},
	publisher = {SciTePress},
	author = {Reinders, Roelof and ten Teije, Annette and Huang, Zisheng},
	year = {2015},
	keywords = {Evidence-based medicine, Medical guideline updates, Medical guidelines},
	pages = {91--102},
}

@incollection{izquirdo_error_2015,
	title = {Error analysis of {Word} {Sense} {Disambiguation}},
	booktitle = {25th {Meeting} of {Computational} {Linguistics} in the {Netherlands}},
	author = {Izquirdo, R. and Postma, M. C. and Vossen, P. T. J. M.},
	year = {2015},
}

@article{milian_enhancing_2015,
	title = {Enhancing reuse of structured eligibility criteria and supporting their relaxation.},
	volume = {56},
	issn = {1532-0464},
	doi = {10.1016/j.jbi.2015.05.005},
	abstract = {Patient recruitment is one of the most important barriers to successful completion of clinical trials and thus to obtaining evidence about new methods for prevention, diagnostics and treatment. The reason is that recruitment is effort consuming. It requires the identification of candidate patients for the trial (the population under study), and verifying for each patient whether the eligibility criteria are met. The work we describe in this paper aims to support the comparison of population under study in different trials, and the design of eligibility criteria for new trials. We do this by introducing structured eligibility criteria, that enhance reuse of criteria across trials. We developed a method that allows for automated structuring of criteria from text. Additionally, structured eligibility criteria allow us to propose suggestions for relaxation of criteria to remove potentially unnecessarily restrictive conditions. We thereby increase the recruitment potential and generalizability of a trial.Our method for automated structuring of criteria enables us to identify related conditions and to compare their restrictiveness. The comparison is based on the general meaning of criteria, comprised of commonly occurring contextual patterns, medical concepts and constraining values. These are automatically identified using our pattern detection algorithm, state of the art ontology annotators and semantic taggers. The comparison uses predefined relations between the patterns, concept equivalences defined in medical ontologies, and threshold values. The result is a library of structured eligibility criteria which can be browsed using fine grained queries. Furthermore, we developed visualizations for the library that enable intuitive navigation of relations between trials, criteria and concepts. These visualizations expose interesting co-occurrences and correlations, potentially enhancing meta-research.The method for criteria structuring processes only certain types of criteria, which results in low recall of the method (18\%) but a high precision for the relations we identify between the criteria (94\%). Analysis of the approach from the medical perspective revealed that the approach can be beneficial for supporting trial design, though more research is needed.},
	journal = {Journal of Biomedical Informatics},
	author = {Milian, K. and Hoekstra, Rinke and Bucur, A. and ten Teije, A.C.M. and van Harmelen, F.A.H. and Paulissen, J.},
	year = {2015},
	note = {Publisher: Academic Press Inc.},
	pages = {205--219},
}

@incollection{hu_detecting_2015,
	title = {Detecting {New} {Evidence} for {Evidence}-based {Guidelines} {Using} a {Semantic} {Distance} {Method}},
	booktitle = {Proceedings 15th {Conference} on {Artificial} {Intelligence} in {Medicine}},
	publisher = {Springer},
	author = {Hu, Q. and Huang, Z. and ten Teije, A.C.M. and van Harmelen, F.A.H.},
	year = {2015},
	pages = {307--316},
}

@article{tarasowa_crowdlearn_2015,
	title = {{CrowdLearn}: {Crowd}-sourcing the {Creation} of {Highly}-structured e-{Learning} {Content}},
	volume = {5},
	issn = {2192-4880},
	doi = {10.3991/ijep.v5i4.4951},
	number = {4},
	journal = {International Journal of Engineering Pedagogy},
	author = {Tarasowa, D. and Khalili, A. and Auer, S.},
	year = {2015},
	note = {Publisher: International Association of Online Engineering},
	pages = {47--54},
}

@article{beek_frank_2015-2,
	title = {Frank: {The} {LOD} cloud at your fingertips?},
	volume = {1361},
	issn = {1613-0073},
	abstract = {Large-scale, algorithmic access to LOD Cloud data has been hampered by the absence of queryable endpoints for many datasets, a plethora of serialization formats, and an abundance of idiosyncrasies such as syntax errors. As of late, very large-scale - hundreds of thousands of document, tens of billions of triples - access to RDF data has become possible thanks to the LOD Laundromat Web Service. In this paper we showcase Frank, a command-line interface to a very large collection of standards-compliant, real-world RDF data that can be used to run Semantic Web experiments and stress-test Linked Data applications.},
	journal = {CEUR workshop proceedings},
	author = {Beek, Wouter and Rietveld, Laurens},
	year = {2015},
	note = {Publisher: CEUR Workshop Proceedings},
	pages = {41--46},
}

@article{magliacane_foxpsl_2015-1,
	title = {{foxPSL}: {A} {Fast}, {Optimized} and {eXtended} {PSL} implementation},
	issn = {0888-613X},
	doi = {10.1016/j.ijar.2015.05.012},
	abstract = {In this paper, we describe foxPSL, a fast, optimized and extended implementation of Probabilistic Soft Logic (PSL) based on the distributed graph processing framework Signal/Collect. PSL is one of the leading formalisms of statistical relational learning, a recently developed field of machine learning that aims at representing both uncertainty and rich relational structures, usually by combining logical representations with probabilistic graphical models. PSL can be seen as both a probabilistic logic and a template language for hinge-loss Markov Random Fields, a type of continuous Markov Random fields (MRF) in which Maximum a Posteriori inference is very efficient, since it can be formulated as a constrained convex minimization problem, as opposed to a discrete optimization problem for standard MRFs. From the logical perspective, a key feature of PSL is the capability to represent soft truth values, allowing the expression of complex domain knowledge, like degrees of truth, in parallel with uncertainty. foxPSL supports the full PSL pipeline from problem definition to a distributed solver that implements the Alternating Direction Method of Multipliers (ADMM) consensus optimization. It provides a Domain Specific Language that extends standard PSL with a class system and existential quantifiers, allowing for efficient grounding. Moreover, it implements a series of configurable optimizations, like optimized grounding of constraints and lazy inference, that improve grounding and inference time. We perform an extensive evaluation, comparing the performance of foxPSL to a state-of-the-art implementation of ADMM consensus optimization in GraphLab, and show an improvement in both inference time and solution quality. Moreover, we evaluate the impact of the optimizations on the execution time and discuss the trade-offs related to each optimization.},
	journal = {International Journal of Approximate Reasoning},
	author = {Magliacane, S. and Stutz, P. and Groth, P. and Bernstein, A.},
	year = {2015},
	note = {Publisher: Elsevier Inc.},
}

@book{israel_depicting_2015,
	title = {Depicting and selecting preferable approaches to knowledge clustering - {RISIS} deliverable},
	publisher = {SNI},
	author = {Israel, E. and Villard, L. and Revollo, M. and Getz, D. and Segal, V. and Larédo, P. and Khalili, A. and Loizou, A. and van den Besselaar, P.A.A. and Veglio, V. and Lepori, B. and Reale, E.},
	year = {2015},
}

@book{wilcke_ariadne_2015,
	series = {Ariadne},
	title = {{ARIADNE}: {First} {Report} on {Data} {Mining}},
	abstract = {Recent years have witnessed a growing interest from archaeological communities in Linked Data. ARIADNE, the AdvancedResearch Infrastructure for Archaeological Data set Networking in Europe, facilitates a central web portal that providesaccess to archaeological data from various sources. Parts of these data have been being published as Linked Data, andare currently available in the Linked Open Data cloud. With it, the nature of these data has shifted from unstructuredto structured. This presents new opportunities for data mining. In this work, we investigate to what extend data mining can contribute to the understanding of linked archaeological data, and which form would best meet the communities' needs.},
	number = {D16.1},
	publisher = {Ariadne},
	author = {Wilcke, W. X. and de Boer, Viktor and van Harmelen, F.A.H. and de Kleijn, Mauritius and Wansleeben, M.},
	year = {2015},
}

@incollection{wang_cognition-inspired_2015,
	title = {Cognition-inspired route evaluation using mobile phone data},
	abstract = {With the increasing popularity of mobile phones, large amounts of real and reliable mobile phone data are being generated every day. These mobile phone data represent the practical travel routes of users and imply the intelligence of them in selecting a suitable route. Usually, an experienced user knows which route is congested in a specified period of time but unblocked in another period of time. Moreover, a route used frequently and recently by a user is usually the suitable one to satisfy the user’s needs. Adaptive control of thought-rational (ACT-R) is a computational cognitive architecture, which provides a good framework to understand the principles and mechanisms of information organization, retrieval and selection in human memory. In this paper, we employ ACT-R to model the process of selecting a suitable route of users. We propose a cognition-inspired route evaluation method to mine the intelligence of users in selecting a suitable route, evaluate the suitability of the routes, and then recommend an ordered list of routes for subscribers. Experiments show that it is effective and feasible to evaluate the suitability of the routes inspired by cognition.},
	booktitle = {Journal of {Natural} {Computing}},
	author = {Wang, H. and Huang, J. and Zhou, E. and Huang, Z. and Zhong, N.},
	year = {2015},
	doi = {10.1007/s11047-014-9479-9},
	pages = {637--648},
}

@incollection{wang_automatic_2015,
	series = {4},
	title = {Automatic {Verification} of {Road} {Signs} based on {Ontology}},
	booktitle = {Journal of {Wuhan} {University}},
	author = {Wang, D. and Huang, Z. and Zhong, N. and Xu, D. and Zhang, X. and Wang, Zhi},
	year = {2015},
	pages = {384--392},
}

@incollection{li_semantic_2015,
	title = {A {Semantic} {Smart} {Hospital} {Information} {System} for {Mental} {Disorders}},
	booktitle = {Proceedings of the 2015 {IEEE}/{WIC}/{ACM} {International} {Conference} on {Web} {Intelligence} ({WI2015})},
	author = {Li, Youjun and Wan, Z. and Huang, J. and Chen, J. and Huang, Z. and Zhong, N.},
	year = {2015},
}

@incollection{wang_intelligent_2015,
	title = {An {Intelligent} {Monitoring} {System} for the {Safety} of {Building} {Structure} under the {W2T} {Framework}},
	abstract = {Monitoring systems for the safety of building structure (SBS) can provide people with important data related to main supporting points in a building and then help people to make a reasonable maintenance schedule. However, more and more data bring a challenge for data management and data mining. In order to meet this challenge, under the framework of Wisdom Web of Things (W2T), we design a monitoring system for the SBS by using the semantic and the multisource data fusion technologies. This system establishes a dynamical data cycle among the physical world (buildings), the social world (humans), and the cyber world (computers) and provides various services in the monitoring process to alleviate engineers' workload. Furthermore, all data in the cyber world are organized as the raw data, the semantic information, and the multisource knowledge. Based on this organization, we can concentrate on the data fusion from the viewpoints of time, space, and multisensor. At last, a prototype system powered by the semantic platform LarKC is tested from the aspects of sample performance and time consumption. In particular, noisy data (i.e., inconsistent, abnormal, or error data) are detected through the fusion of multisource knowledge, and some rule-based reasoning is conducted to provide personalized service.},
	booktitle = {Journal of {Distributed} {Sensor} {Networks}},
	author = {Wang, H. and Huang, Z. and Zhong, N. and Han, Y. and Zhang, F.},
	year = {2015},
	doi = {10.1155/2015/378694},
}

@incollection{bazoubandi_compact_2015,
	series = {Lecture {Notes} in {Computer} {Science} (including subseries {Lecture} {Notes} in {Artificial} {Intelligence} and {Lecture} {Notes} in {Bioinformatics})},
	title = {A {Compact} {In}-{Memory} {Dictionary} for {RDF} data},
	volume = {9088},
	abstract = {While almost all dictionary compression techniques focus on static RDF data, we present a compact in-memory RDF dictionary for dynamic and streaming data. To do so, we analysed the structure of terms in real-world datasets and observed a high degree of common prefixes. We studied the applicability of Trie data structures on RDF data to reduce the memory occupied by common prefixes and discovered that all existing Trie implementations lead to either poor performance, or an excessive memory wastage. In our approach, we address the existing limitations of Tries for RDF data, and propose a new variant of Trie which contains some optimizations explicitly designed to improve the performance on RDF data. Furthermore, we show how we use this Trie as an in-memory dictionary by using as numerical ID a memory address instead of an integer counter. This design removes the need for an additional decoding data structure, and further reduces the occupied memory. An empirical analysis on realworld datasets shows that with a reasonable overhead our technique uses 50–59\% less memory than a conventional uncompressed dictionary.},
	booktitle = {The {Semantic} {Web}: {Latest} {Advances} and {New} {Domains} - 12th {European} {Semantic} {Web} {Conference}, {ESWC} 2015, {Proceedings}},
	publisher = {Springer/Verlag},
	author = {Bazoubandi, Hamid R. and de Rooij, Steven and Urbani, Jacopo and ten Teije, Annette and van Harmelen, Frank and Bal, Henri},
	year = {2015},
	doi = {10.1007/978-3-319-18818-8_13},
	pages = {205--220},
}

@article{janowicz_why_2015,
	title = {Why the {Data} {Train} {Needs} {Semantic} {Rails}},
	volume = {36},
	issn = {0738-4602},
	abstract = {While catchphrases such as big data, smart data, data-intensive science, or smart dust highlight different aspects, they share a common theme - namely, a shift toward a data-centered perspective in which the synthesis and analysis of data at an ever-increasing spatial, temporal, and thematic resolution promise new insights, while, at the same time, reduce the need for strong domain theories as starting points. In terms of the envisioned methodologies, those catchphrases tend to emphasize the role of predictive analytics, that is, statistical techniques including data mining and machine learning, as well as supercomputing. Interestingly, however, while this perspective takes the availability of data as a given, it does not answer the question how one would discover the required data in today's chaotic information universe, how one would understand which data sets can be meaningfully integrated, and how to communicate the results to humans and machines alike. The semantic web addresses these questions. In the following, we argue why the data train needs semantic rails. We point out that making sense of data and gaining new insights work best if inductive and deductive techniques go hand-in-hand instead of competing over the prerogative of interpretation.},
	number = {1},
	journal = {The AI Magazine},
	author = {Janowicz, Krzysztof and Hitzler, Pascal and Hendler, James A. and van Harmelen, Frank},
	month = mar,
	year = {2015},
	note = {Publisher: AI Access Foundation},
	pages = {5--14},
}
